# Interrater Reliability Analysis
This folder contains an R script and a CSV file to calculate the interrater reliability of the two raters that performed the study selection.
We performed four calibration rounds Ã  100 resources.
For the final round, the percentage agreement was 87% and Cohen's Kappa was 0.602.

`0` means excluded and `1` means included.
For sources, `G` means Google, `B` Bing, `SO` StackOverflow, and `SE` StackExchange.